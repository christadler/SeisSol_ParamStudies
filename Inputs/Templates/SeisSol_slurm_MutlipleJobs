#!/bin/bash
# Job Name and Files (also --job-name)

#SBATCH -J MathildeTest_multi
#Output and error (also --output, --error):
#SBATCH -o ./%j.%x.out
#SBATCH -e ./%j.%x.err

#Initial working directory:
#SBATCH --chdir=/hppfs/work/pn49ha/ra35zih2/MathildeTest/

#Notification and type
#SBATCH --mail-type=END
#SBATCH --mail-user=marchandon@geophysik.uni-muenchen.de

# Wall clock limit:
# 6 runs, 40 min each, 240 min = 4h
#SBATCH --time=04:00:00
#SBATCH --no-requeue

#Setup of execution environment
#SBATCH --export=ALL
#SBATCH --account=pn49ha
#constraints are optional
#--constraint="scratch&work"
# micro partition for tests, general for production
#SBATCH --partition=micro

#Number of nodes and MPI tasks per node:
# --nodes=48
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
module load slurm_setup

#SBATCH --ear=off

#Run the program:
export MP_SINGLE_THREAD=no
unset KMP_AFFINITY
export OMP_NUM_THREADS=94
export OMP_PLACES="cores(47)"

export XDMFWRITER_ALIGNMENT=8388608
export XDMFWRITER_BLOCK_SIZE=8388608
export SC_CHECKPOINT_ALIGNMENT=8388608

export SEISSOL_CHECKPOINT_ALIGNMENT=8388608
export SEISSOL_CHECKPOINT_DIRECT=1
export ASYNC_MODE=THREAD
export ASYNC_BUFFER_ALIGNMENT=8388608
export SEISSOL_ASAGI_MPI_MODE=OFF

source /etc/profile.d/modules.sh

echo 'num_nodes:' $SLURM_JOB_NUM_NODES 'ntasks:' $SLURM_NTASKS 'cpus_per_task:' $SLURM_CPUS_PER_TASK
ulimit -Ss 2097152
cd 0014/
mpiexec -n $SLURM_NTASKS temp_seissol/SeisSol_Release_dskx_4_elastic parameters.par
cd ../
cd 0016/
mpiexec -n $SLURM_NTASKS temp_seissol/SeisSol_Release_dskx_4_elastic parameters.par
cd ../
cd 0018/
mpiexec -n $SLURM_NTASKS temp_seissol/SeisSol_Release_dskx_4_elastic parameters.par
cd ..
cd 0020/
mpiexec -n $SLURM_NTASKS temp_seissol/SeisSol_Release_dskx_4_elastic parameters.par
cd ..
cd 0021/
mpiexec -n $SLURM_NTASKS temp_seissol/SeisSol_Release_dskx_4_elastic parameters.par
cd ..
cd 0023/
mpiexec -n $SLURM_NTASKS temp_seissol/SeisSol_Release_dskx_4_elastic parameters.par
cd ..
#wait